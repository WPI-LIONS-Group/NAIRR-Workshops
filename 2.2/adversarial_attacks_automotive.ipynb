{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40810352",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962dc34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install numpy --upgrade --only-binary :all:\n",
    "%pip install adversarial-robustness-toolbox torch torchvision pillow matplotlib opencv-python --only-binary :all:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fbd360",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac509f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent, CarliniL2Method\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f4006",
   "metadata": {},
   "source": [
    "## 3. Load Pre-trained Model and Labels\n",
    "\n",
    "We'll use MobileNetV2 trained on ImageNet, which includes many automotive classes (cars, trucks, buses, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb68f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MobileNetV2 model\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Download ImageNet labels\n",
    "IMAGENET_LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "with urllib.request.urlopen(IMAGENET_LABELS_URL) as url:\n",
    "    imagenet_labels = json.loads(url.read().decode())\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f72b595",
   "metadata": {},
   "source": [
    "## 4. Load Real Automotive Images\n",
    "\n",
    "Download and prepare real automotive images for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs for sample automotive images\n",
    "image_urls = [\n",
    "    \"https://images.unsplash.com/photo-1549317661-bd32c8ce0db2?w=800&q=80\",  # Sports car\n",
    "    \"https://images.unsplash.com/photo-1583121274602-3e2820c69888?w=800&q=80\",  # SUV\n",
    "    \"https://images.unsplash.com/photo-1552519507-da3b142c6e3d?w=800&q=80\",  # Sedan\n",
    "]\n",
    "\n",
    "def download_and_prepare_image(url, max_size=(224, 224)):\n",
    "    \"\"\"Download image from URL and resize it\"\"\"\n",
    "    try:\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            img_array = np.asarray(bytearray(response.read()), dtype=np.uint8)\n",
    "            img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            # Resize while maintaining aspect ratio\n",
    "            h, w = img_rgb.shape[:2]\n",
    "            scale = min(max_size[0]/h, max_size[1]/w)\n",
    "            new_h, new_w = int(h*scale), int(w*scale)\n",
    "            img_resized = cv2.resize(img_rgb, (new_w, new_h))\n",
    "            return img_resized\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Download all images\n",
    "print(\"Downloading automotive images...\")\n",
    "sample_images = []\n",
    "for i, url in enumerate(image_urls):\n",
    "    img = download_and_prepare_image(url)\n",
    "    if img is not None:\n",
    "        sample_images.append(img)\n",
    "        print(f\"âœ“ Image {i+1} downloaded successfully\")\n",
    "\n",
    "print(f\"\\nTotal images loaded: {len(sample_images)}\")\n",
    "\n",
    "# Display all images\n",
    "fig, axes = plt.subplots(1, len(sample_images), figsize=(15, 5))\n",
    "if len(sample_images) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (img, ax) in enumerate(zip(sample_images, axes)):\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Automotive Image {idx+1}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cdfdb2",
   "metadata": {},
   "source": [
    "## 5. Preprocess Images and Get Predictions\n",
    "\n",
    "Preprocess all images and get model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5081803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(img_array):\n",
    "    \"\"\"Preprocess image for PyTorch MobileNetV2\"\"\"\n",
    "    # Convert BGR to RGB if needed\n",
    "    img_rgb = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB) if len(img_array.shape) == 3 else img_array\n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    \n",
    "    # PyTorch preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    img_tensor = preprocess(img_pil)\n",
    "    return img_tensor.unsqueeze(0).numpy()\n",
    "\n",
    "def get_predictions(model, img_tensor, top_k=3):\n",
    "    \"\"\"Get top predictions from PyTorch model\"\"\"\n",
    "    with torch.no_grad():\n",
    "        img_torch = torch.from_numpy(img_tensor)\n",
    "        outputs = model(img_torch)\n",
    "        probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "    \n",
    "    top_prob, top_idx = torch.topk(probabilities, top_k)\n",
    "    results = [(idx.item(), imagenet_labels[idx.item()], prob.item()) \n",
    "               for idx, prob in zip(top_idx, top_prob)]\n",
    "    return results\n",
    "\n",
    "# Preprocess all images and get predictions\n",
    "preprocessed_images = []\n",
    "all_predictions = []\n",
    "\n",
    "print(\"Processing images and getting predictions...\\n\")\n",
    "for idx, img in enumerate(sample_images):\n",
    "    x_test = load_and_preprocess_image(img)\n",
    "    preprocessed_images.append(x_test)\n",
    "    \n",
    "    decoded_preds = get_predictions(model, x_test, top_k=3)\n",
    "    all_predictions.append(decoded_preds)\n",
    "    \n",
    "    print(f\"Image {idx+1} - Original Predictions:\")\n",
    "    for i, (label_idx, label, score) in enumerate(decoded_preds):\n",
    "        print(f\"  {i+1}. {label}: {score*100:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ccbb4",
   "metadata": {},
   "source": [
    "## 6. Wrap Model for ART\n",
    "\n",
    "Wrap the TensorFlow model with ART's classifier wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb65591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Wrap the model with ART\n",
    "classifier = PyTorchClassifier(\n",
    "    model=model,\n",
    "    loss=loss_fn,\n",
    "    input_shape=(3, 224, 224),\n",
    "    nb_classes=1000,\n",
    "    clip_values=(-3, 3)  # Normalized range for PyTorch\n",
    ")\n",
    "\n",
    "print(\"Model wrapped successfully for ART!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c25c563",
   "metadata": {},
   "source": [
    "## 7. Attack 1: Fast Gradient Sign Method (FGSM)\n",
    "\n",
    "FGSM is a simple and fast attack that perturbs the image in the direction of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4b134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FGSM attack\n",
    "fgsm_attack = FastGradientMethod(estimator=classifier, eps=0.3)\n",
    "\n",
    "# Generate adversarial examples for all images\n",
    "print(\"Generating FGSM adversarial examples...\\n\")\n",
    "fgsm_adversarial_images = []\n",
    "fgsm_predictions = []\n",
    "\n",
    "for idx, x_test in enumerate(preprocessed_images):\n",
    "    # Generate adversarial example\n",
    "    x_test_adv = fgsm_attack.generate(x=x_test)\n",
    "    fgsm_adversarial_images.append(x_test_adv)\n",
    "    \n",
    "    # Get predictions on adversarial image\n",
    "    decoded_preds_adv = get_predictions(model, x_test_adv, top_k=3)\n",
    "    fgsm_predictions.append(decoded_preds_adv)\n",
    "    \n",
    "    print(f\"Image {idx+1} - FGSM Adversarial Predictions:\")\n",
    "    for i, (label_idx, label, score) in enumerate(decoded_preds_adv):\n",
    "        print(f\"  {i+1}. {label}: {score*100:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea436a",
   "metadata": {},
   "source": [
    "### Visualize FGSM Attack Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6b0b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_pytorch(img_tensor):\n",
    "    \"\"\"Denormalize PyTorch tensor for display\"\"\"\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = img_tensor[0].transpose(1, 2, 0)\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img * 255, 0, 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "# Visualize FGSM results for all images\n",
    "fig, axes = plt.subplots(len(sample_images), 3, figsize=(15, 5*len(sample_images)))\n",
    "if len(sample_images) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for idx in range(len(sample_images)):\n",
    "    # Original image\n",
    "    axes[idx, 0].imshow(sample_images[idx])\n",
    "    orig_pred = all_predictions[idx][0]\n",
    "    axes[idx, 0].set_title(f\"Original Image {idx+1}\\nTop: {orig_pred[1]}\\n({orig_pred[2]*100:.1f}%)\")\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    # Adversarial image\n",
    "    adv_img_display = denormalize_pytorch(fgsm_adversarial_images[idx])\n",
    "    axes[idx, 1].imshow(adv_img_display)\n",
    "    adv_pred = fgsm_predictions[idx][0]\n",
    "    axes[idx, 1].set_title(f\"FGSM Attack (eps=0.3)\\nTop: {adv_pred[1]}\\n({adv_pred[2]*100:.1f}%)\")\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    # Perturbation (amplified for visibility)\n",
    "    perturbation = (fgsm_adversarial_images[idx][0] - preprocessed_images[idx][0]).transpose(1, 2, 0) * 50\n",
    "    perturbation = np.clip(perturbation + 128, 0, 255).astype(np.uint8)\n",
    "    axes[idx, 2].imshow(perturbation)\n",
    "    axes[idx, 2].set_title(\"Perturbation\\n(50x amplified)\")\n",
    "    axes[idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45e342",
   "metadata": {},
   "source": [
    "## 8. Attack 2: Projected Gradient Descent (PGD)\n",
    "\n",
    "PGD is an iterative attack that is more powerful than FGSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28508462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PGD attack\n",
    "pgd_attack = ProjectedGradientDescent(\n",
    "    estimator=classifier,\n",
    "    eps=0.3,\n",
    "    eps_step=0.03,\n",
    "    max_iter=20,\n",
    "    targeted=False\n",
    ")\n",
    "\n",
    "# Generate adversarial examples for all images\n",
    "print(\"Generating PGD adversarial examples...\\n\")\n",
    "pgd_adversarial_images = []\n",
    "pgd_predictions = []\n",
    "\n",
    "for idx, x_test in enumerate(preprocessed_images):\n",
    "    # Generate adversarial example\n",
    "    x_test_adv = pgd_attack.generate(x=x_test)\n",
    "    pgd_adversarial_images.append(x_test_adv)\n",
    "    \n",
    "    # Get predictions on adversarial image\n",
    "    decoded_preds_adv = get_predictions(model, x_test_adv, top_k=3)\n",
    "    pgd_predictions.append(decoded_preds_adv)\n",
    "    \n",
    "    print(f\"Image {idx+1} - PGD Adversarial Predictions:\")\n",
    "    for i, (label_idx, label, score) in enumerate(decoded_preds_adv):\n",
    "        print(f\"  {i+1}. {label}: {score*100:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a570165c",
   "metadata": {},
   "source": [
    "### Visualize PGD Attack Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e682c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PGD results for all images\n",
    "fig, axes = plt.subplots(len(sample_images), 3, figsize=(15, 5*len(sample_images)))\n",
    "if len(sample_images) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for idx in range(len(sample_images)):\n",
    "    # Original image\n",
    "    axes[idx, 0].imshow(sample_images[idx])\n",
    "    orig_pred = all_predictions[idx][0]\n",
    "    axes[idx, 0].set_title(f\"Original Image {idx+1}\\nTop: {orig_pred[1]}\\n({orig_pred[2]*100:.1f}%)\")\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    # Adversarial image\n",
    "    adv_img_display = denormalize_pytorch(pgd_adversarial_images[idx])\n",
    "    axes[idx, 1].imshow(adv_img_display)\n",
    "    adv_pred = pgd_predictions[idx][0]\n",
    "    axes[idx, 1].set_title(f\"PGD Attack (eps=0.3)\\nTop: {adv_pred[1]}\\n({adv_pred[2]*100:.1f}%)\")\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    # Perturbation (amplified for visibility)\n",
    "    perturbation = (pgd_adversarial_images[idx][0] - preprocessed_images[idx][0]).transpose(1, 2, 0) * 50\n",
    "    perturbation = np.clip(perturbation + 128, 0, 255).astype(np.uint8)\n",
    "    axes[idx, 2].imshow(perturbation)\n",
    "    axes[idx, 2].set_title(\"Perturbation\\n(50x amplified)\")\n",
    "    axes[idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9293bbfd",
   "metadata": {},
   "source": [
    "## 9. Attack 3: Carlini & Wagner L2 Attack\n",
    "\n",
    "C&W attack is a powerful optimization-based attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a071f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create C&W attack (using fewer iterations for speed)\n",
    "cw_attack = CarliniL2Method(\n",
    "    classifier=classifier,\n",
    "    confidence=0.0,\n",
    "    targeted=False,\n",
    "    max_iter=10,  # Reduced for speed\n",
    "    learning_rate=0.01\n",
    ")\n",
    "\n",
    "# Generate adversarial examples for all images\n",
    "print(\"Generating C&W adversarial examples (this may take a moment)...\\n\")\n",
    "cw_adversarial_images = []\n",
    "cw_predictions = []\n",
    "\n",
    "for idx, x_test in enumerate(preprocessed_images):\n",
    "    print(f\"Processing Image {idx+1}...\")\n",
    "    # Generate adversarial example\n",
    "    x_test_adv = cw_attack.generate(x=x_test)\n",
    "    cw_adversarial_images.append(x_test_adv)\n",
    "    \n",
    "    # Get predictions on adversarial image\n",
    "    decoded_preds_adv = get_predictions(model, x_test_adv, top_k=3)\n",
    "    cw_predictions.append(decoded_preds_adv)\n",
    "    \n",
    "    print(f\"Image {idx+1} - C&W Adversarial Predictions:\")\n",
    "    for i, (label_idx, label, score) in enumerate(decoded_preds_adv):\n",
    "        print(f\"  {i+1}. {label}: {score*100:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f62e5",
   "metadata": {},
   "source": [
    "### Visualize C&W Attack Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6ccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize C&W results for all images\n",
    "fig, axes = plt.subplots(len(sample_images), 3, figsize=(15, 5*len(sample_images)))\n",
    "if len(sample_images) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for idx in range(len(sample_images)):\n",
    "    # Original image\n",
    "    axes[idx, 0].imshow(sample_images[idx])\n",
    "    orig_pred = all_predictions[idx][0]\n",
    "    axes[idx, 0].set_title(f\"Original Image {idx+1}\\nTop: {orig_pred[1]}\\n({orig_pred[2]*100:.1f}%)\")\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    # Adversarial image\n",
    "    adv_img_display = denormalize_pytorch(cw_adversarial_images[idx])\n",
    "    axes[idx, 1].imshow(adv_img_display)\n",
    "    adv_pred = cw_predictions[idx][0]\n",
    "    axes[idx, 1].set_title(f\"C&W Attack\\nTop: {adv_pred[1]}\\n({adv_pred[2]*100:.1f}%)\")\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    # Perturbation (amplified for visibility)\n",
    "    perturbation = (cw_adversarial_images[idx][0] - preprocessed_images[idx][0]).transpose(1, 2, 0) * 50\n",
    "    perturbation = np.clip(perturbation + 128, 0, 255).astype(np.uint8)\n",
    "    axes[idx, 2].imshow(perturbation)\n",
    "    axes[idx, 2].set_title(\"Perturbation\\n(50x amplified)\")\n",
    "    axes[idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55a791",
   "metadata": {},
   "source": [
    "## 10. Compare All Attacks Side-by-Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ba2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all attacks side-by-side for each image\n",
    "for img_idx in range(len(sample_images)):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    \n",
    "    # Row 1: Images\n",
    "    # Original\n",
    "    axes[0, 0].imshow(sample_images[img_idx])\n",
    "    axes[0, 0].set_title(f\"Original Image {img_idx+1}\", fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    # FGSM\n",
    "    fgsm_display = denormalize_pytorch(fgsm_adversarial_images[img_idx])\n",
    "    axes[0, 1].imshow(fgsm_display)\n",
    "    axes[0, 1].set_title(\"FGSM Attack\", fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # PGD\n",
    "    pgd_display = denormalize_pytorch(pgd_adversarial_images[img_idx])\n",
    "    axes[0, 2].imshow(pgd_display)\n",
    "    axes[0, 2].set_title(\"PGD Attack\", fontsize=12, fontweight='bold')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    # C&W\n",
    "    cw_display = denormalize_pytorch(cw_adversarial_images[img_idx])\n",
    "    axes[0, 3].imshow(cw_display)\n",
    "    axes[0, 3].set_title(\"C&W Attack\", fontsize=12, fontweight='bold')\n",
    "    axes[0, 3].axis('off')\n",
    "    \n",
    "    # Row 2: Predictions\n",
    "    predictions_data = [\n",
    "        (\"Original\", all_predictions[img_idx]),\n",
    "        (\"FGSM\", fgsm_predictions[img_idx]),\n",
    "        (\"PGD\", pgd_predictions[img_idx]),\n",
    "        (\"C&W\", cw_predictions[img_idx])\n",
    "    ]\n",
    "    \n",
    "    for idx, (title, preds) in enumerate(predictions_data):\n",
    "        text = \"Top 3 Predictions:\\n\\n\"\n",
    "        for i, (class_idx, label, score) in enumerate(preds):\n",
    "            text += f\"{i+1}. {label}\\n   {score*100:.1f}%\\n\\n\"\n",
    "        \n",
    "        axes[1, idx].text(0.1, 0.9, text, fontsize=10, verticalalignment='top',\n",
    "                         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "        axes[1, idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a5f54a",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "1. **FGSM (Fast Gradient Sign Method)**:\n",
    "   - Fastest attack\n",
    "   - Single-step gradient-based\n",
    "   - Creates visible but effective perturbations\n",
    "\n",
    "2. **PGD (Projected Gradient Descent)**:\n",
    "   - Iterative version of FGSM\n",
    "   - More powerful than FGSM\n",
    "   - Better at fooling the model\n",
    "\n",
    "3. **C&W (Carlini & Wagner)**:\n",
    "   - Optimization-based attack\n",
    "   - Can create minimal perturbations\n",
    "   - Slower but very effective\n",
    "\n",
    "### Automotive AI Implications:\n",
    "\n",
    "- These attacks demonstrate vulnerabilities in image classification models\n",
    "- Critical for autonomous driving systems that rely on visual perception\n",
    "- Small perturbations can cause misclassification of vehicles, signs, or pedestrians\n",
    "- Robustness testing is essential for safety-critical automotive AI systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
